{
  "title": "TransMLA: Multi-head latent attention is all you need",
  "rank": "23.",
  "href": "https://arxiv.org/abs/2502.07864"
}